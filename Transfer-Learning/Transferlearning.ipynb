{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transferlearning.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "mqcjjpnB-AMs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Transfer Learning (Tensorflow + VGG16 + CIFAR10)\n",
        "The code below performs a complete task of transfer learning. All of it was made thinking of an easy way to learn this subject and easy way to modify it in order to resolve other tasks.\n"
      ]
    },
    {
      "metadata": {
        "id": "E3v8U2KBDmby",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###All the necessary imports\n",
        "Note that this code was made for running on [Google Colab](https://colab.research.google.com/notebooks/welcome.ipynb). Then, its usage outside this plataform requires adaptations. As taking off all the Google Colab dependencies and download manually the VGG16 model and put it into the folder \"./model\". The model can be downloaded [here](https://github.com/ry/tensorflow-vgg16/blob/master/vgg16-20160129.tfmodel.torrent):"
      ]
    },
    {
      "metadata": {
        "id": "AmDLc8t7WWL6",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        },
        "cellView": "both"
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline  \n",
        "import pickle\n",
        "import numpy as np\n",
        "import os\n",
        "from urllib.request import urlretrieve\n",
        "import tarfile\n",
        "import zipfile\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from time import time\n",
        "import skimage as sk\n",
        "from skimage import transform\n",
        "from skimage import util\n",
        "import random\n",
        "import math\n",
        "import os.path\n",
        "from random import shuffle\n",
        "import logging\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from google.colab import files\n",
        "from itertools import product\n",
        "!pip install googledrivedownloader\n",
        "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JgYef7Lv-Ljr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### Class that defines the principals hyperparameters used by the model\n"
      ]
    },
    {
      "metadata": {
        "id": "uIuAiX69WXn8",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class Hyperparameters:\n",
        "  def __init__(self):\n",
        "      self.image_size = 32\n",
        "      self.image_channels = 3\n",
        "      self.num_classes = 10\n",
        "      self.initial_learning_rate = 1e-4\n",
        "      self.decay_steps = 1e3\n",
        "      self.decay_rate = 0.98\n",
        "      self.cut_layer = \"pool5\"\n",
        "      self.hidden_layers = [512]\n",
        "      self.batch_size = 128\n",
        "      self.num_epochs = 200\n",
        "      self.check_points_path= \"./tensorboard/cifar10_vgg16\"\n",
        "      self.keep = 1.0\n",
        "      self.fine_tunning = False\n",
        "      self.bottleneck = True\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8sB2nJT3-mli",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Class that  provides same utilities for the model, such as downloads files,  gets dataset, does  data augmentation,  generates bottlenecks files and creates a confusion matrix from the model."
      ]
    },
    {
      "metadata": {
        "id": "lqSlsfGaug9m",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class utils:\n",
        "      def get_or_generate_bottleneck( sess, model, file_name, dataset, labels, batch_size = 128):\n",
        "\n",
        "          path_file = os.path.join(\"./data_set\",file_name+\".pkl\")\n",
        "          if(os.path.exists(path_file)):\n",
        "                print(\"Loading bottleneck from \\\"{}\\\" \".format(path_file))\n",
        "                with open(path_file, 'rb') as f:\n",
        "                   return pickle.load(f)\n",
        "\n",
        "          bottleneck_data = []\n",
        "          original_labels = []\n",
        "\n",
        "          print(\"Generating Bottleneck \\\"{}.pkl\\\" \".format(file_name) )\n",
        "          count = 0\n",
        "          amount = len(labels) // batch_size\n",
        "          indices = list(range(len(labels)))\n",
        "          for i in range(amount+1):\n",
        "\n",
        "                if (i+1)*batch_size < len(indices):\n",
        "                  indices_next_batch = indices[i*batch_size: (i+1)*batch_size]\n",
        "                else:\n",
        "                   indices_next_batch = indices[i*batch_size:]\n",
        "                batch_size = len(indices_next_batch)\n",
        "\n",
        "                data = dataset[indices_next_batch]\n",
        "                label = labels[indices_next_batch]\n",
        "                input_size = np.prod(model[\"bottleneck_tensor\"].shape.as_list()[1:])\n",
        "                tensor = sess.run(model[\"bottleneck_tensor\"], feed_dict={model[\"images\"]:data, model[\"bottleneck_input\"]:np.zeros((batch_size,input_size)), model[\"labels\"]:label,model[\"keep\"]:1.0})\n",
        "                for t in range(batch_size):\n",
        "                  bottleneck_data.append(np.squeeze(tensor[t]))\n",
        "                  original_labels.append(np.squeeze(label[t]))\n",
        "          \n",
        "          bottleneck = {\n",
        "              \"data\":np.array(bottleneck_data),\n",
        "              \"labels\":np.array(original_labels)\n",
        "          } \n",
        "          \n",
        "          with open(path_file, 'wb') as f:\n",
        "            pickle.dump(bottleneck, f)\n",
        "\n",
        "\n",
        "          print(\"Done\")   \n",
        "\n",
        "          return bottleneck\n",
        "\n",
        "      def get_data_set(name=\"train\"):\n",
        "          x = None\n",
        "          y = None\n",
        "          folder_name = 'cifar_10'\n",
        "          main_directory = \"./data_set\"\n",
        "          url = \"http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
        "\n",
        "          utils.maybe_download_and_extract(url, main_directory,folder_name, \"cifar-10-batches-py\")\n",
        "\n",
        "\n",
        "          f = open(os.path.join(main_directory,folder_name,\"batches.meta\"), 'rb')\n",
        "          f.close()\n",
        "\n",
        "          if name is \"train\":\n",
        "              for i in range(5):\n",
        "                  f = open('./data_set/'+folder_name+'/data_batch_' + str(i + 1), 'rb')\n",
        "                  datadict = pickle.load(f, encoding='latin1')\n",
        "                  f.close()\n",
        "\n",
        "                  _X = datadict[\"data\"]\n",
        "                  _Y = datadict['labels']\n",
        "\n",
        "                  _X = np.array(_X, dtype=float) / 255.0\n",
        "                  _X = _X.reshape([-1, 3, 32, 32])\n",
        "                  _X = _X.transpose([0, 2, 3, 1])\n",
        "\n",
        "                  if x is None:\n",
        "                      x = _X\n",
        "                      y = _Y\n",
        "                  else:\n",
        "                      x = np.concatenate((x, _X), axis=0)\n",
        "                      y = np.concatenate((y, _Y), axis=0)\n",
        "\n",
        "          elif name is \"test\":\n",
        "              f = open('./data_set/'+folder_name+'/test_batch', 'rb')\n",
        "              datadict = pickle.load(f, encoding='latin1')\n",
        "              f.close()\n",
        "\n",
        "              x = datadict[\"data\"]\n",
        "              y = np.array(datadict['labels'])\n",
        "\n",
        "              x = np.array(x, dtype=float) / 255.0\n",
        "              x = x.reshape([-1, 3, 32, 32])\n",
        "              x = x.transpose([0, 2, 3, 1])\n",
        "\n",
        "          return x, utils._dense_to_one_hot(y)\n",
        "\n",
        "\n",
        "      def _dense_to_one_hot( labels_dense, num_classes=10):\n",
        "          num_labels = labels_dense.shape[0]\n",
        "          index_offset = np.arange(num_labels) * num_classes\n",
        "          labels_one_hot = np.zeros((num_labels, num_classes))\n",
        "          labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n",
        "\n",
        "          return labels_one_hot\n",
        "\n",
        "\n",
        "      \n",
        "\n",
        "\n",
        "      def maybe_download_and_extract( url, main_directory,filename, original_name):\n",
        "          def _print_download_progress( count, block_size, total_size):\n",
        "            pct_complete = float(count * block_size) / total_size\n",
        "            msg = \"\\r --> progress: {0:.1%}\".format(pct_complete)\n",
        "            sys.stdout.write(msg)\n",
        "            sys.stdout.flush()\n",
        "          \n",
        "          if not os.path.exists(main_directory):\n",
        "              os.makedirs(main_directory)\n",
        "              url_file_name = url.split('/')[-1]\n",
        "              zip_file = os.path.join(main_directory,url_file_name)\n",
        "              print(\"Downloading \",url_file_name)\n",
        "\n",
        "              try:\n",
        "                file_path, _ = urlretrieve(url=url, filename= zip_file, reporthook=_print_download_progress)\n",
        "              except:\n",
        "                os.system(\"rm -r \"+main_directory)\n",
        "                print(\"An error occurred while downloading: \",url)\n",
        "\n",
        "                if(original_name == 'vgg16-20160129.tfmodel'):\n",
        "                  print(\"This could be for a problem with github. We will try downloading from the Google Drive\")\n",
        "                  from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "\n",
        "                  gdd.download_file_from_google_drive(file_id='1xJZDLu_TK_SyQz-SaetAL_VOFY7xdAt5',\n",
        "                                                      dest_path='./models/vgg16-20160129.tfmodel',\n",
        "                                                      unzip=False)\n",
        "                else: print(\"This could be for a problem with the storage site. Try again later\")\n",
        "                return\n",
        "\n",
        "              print(\"\\nDownload finished.\")\n",
        "              if file_path.endswith(\".zip\"):\n",
        "                  print( \"Extracting files.\")\n",
        "\n",
        "                  zipfile.ZipFile(file=file_path, mode=\"r\").extractall(main_directory)\n",
        "              elif file_path.endswith((\".tar.gz\", \".tgz\")):\n",
        "                  print( \"Extracting files.\")\n",
        "                  tarfile.open(name=file_path, mode=\"r:gz\").extractall(main_directory)\n",
        "                  os.remove(file_path)\n",
        "\n",
        "              os.rename(os.path.join(main_directory,original_name), os.path.join(main_directory,filename))\n",
        "              print(\"Done.\")\n",
        "     \n",
        "      def data_augmentation(images, labels):\n",
        "        \n",
        "          def random_rotation(image_array):\n",
        "              # pick a random degree of rotation between 25% on the left and 25% on the right\n",
        "              random_degree = random.uniform(-15, 15)\n",
        "              return sk.transform.rotate(image_array, random_degree)\n",
        "\n",
        "          def random_noise(image_array):\n",
        "              # add random noise to the image\n",
        "              return sk.util.random_noise(image_array)\n",
        "\n",
        "          def horizontal_flip(image_array):\n",
        "              # horizontal flip doesn't need skimage, it's easy as flipping the image array of pixels !\n",
        "              return image_array[:, ::-1]\n",
        "          print(\"Augmenting data...\")\n",
        "          aug_images = []\n",
        "          aug_labels = []\n",
        "\n",
        "          aug_images.extend( list(map(random_rotation, images)) )\n",
        "          aug_labels.extend(labels)\n",
        "          aug_images.extend( list(map(random_noise,    images)) )\n",
        "          aug_labels.extend(labels)\n",
        "          aug_images.extend( list(map(horizontal_flip, images)) )\n",
        "          aug_labels.extend(labels)\n",
        "\n",
        "\n",
        "          return np.array(aug_images), np.array(aug_labels)\n",
        "        \n",
        "        \n",
        "        \n",
        "      \n",
        "      def generate_confusion_matrix( predictions, class_names):\n",
        "        \n",
        "        def plot_confusion_matrix(cm, classes,\n",
        "                                    normalize=False,\n",
        "                                    title='Confusion matrix',\n",
        "                                    cmap=plt.cm.Blues):\n",
        "                \"\"\"\n",
        "                This function prints and plots the confusion matrix.\n",
        "                Normalization can be applied by setting `normalize=True`.\n",
        "                \"\"\"\n",
        "                if normalize:\n",
        "                    cm = 100 * cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "                    print(\"Normalized confusion matrix\")\n",
        "                else:\n",
        "                    print('Confusion matrix, without normalization')\n",
        "\n",
        "                print(cm.shape)\n",
        "\n",
        "                plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "                plt.title(title)\n",
        "                plt.colorbar()\n",
        "                \n",
        "                tick_marks = np.arange(len(classes))\n",
        "               \n",
        "          \n",
        "                plt.xticks(tick_marks, classes, rotation=45)\n",
        "                plt.yticks(tick_marks, classes)\n",
        "\n",
        "                fmt = '.2f' if normalize else 'd'\n",
        "                thresh = cm.max() / 2.\n",
        "                symbol = \"%\" if normalize else \"\"\n",
        "                for i, j in product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "                    plt.text(j, i, format(cm[i, j], fmt)+symbol,\n",
        "                            horizontalalignment=\"center\",\n",
        "                            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "                plt.tight_layout()\n",
        "                plt.ylabel('Real')\n",
        "                plt.xlabel('Predicted')\n",
        "        # Compute confusion matrix\n",
        "        cnf_matrix = confusion_matrix(predictions[\"labels\"],predictions[\"classes\"])\n",
        "        np.set_printoptions(precision=2)\n",
        "        \n",
        "\n",
        "        # # Plot normalized confusion matrix\n",
        "        plt.figure(figsize=(10,7))\n",
        "        plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
        "                            title='Normalized confusion matrix')\n",
        "        plt.grid('off')\n",
        "\n",
        "        #plt.savefig(\"./confusion_matrix.png\") #Save the confision matrix as a .png figure.\n",
        "        plt.show()\n",
        "#         "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "i14EUs_7_3ht",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###The function \"get_vgg16\" returns a pretrained vgg16 model.\n",
        "\n",
        "All the work of loading and restoring the weights of the model is responsibility of tensorflow. We just need to choose which layer we want to cut and pass it as parameter for the function \"get_vgg16\". In transfer learning it is common to dispose the fully connected layer and reuse only the convolutional ones. It occurs because the new problem/dataset used to be different from the original (such one that was used for training the model), and the numbers of classes is often different as well.\n",
        "\n",
        "In a CNN, the first layers are responsible for selecting borders, the middle layers for selecting some kinds of patterns, based on combinations of those edges obtained previously and the last ones for composing patterns with a high level of representation, also known as semantic layers. Thereby, when the new dataset is much different of the original, the last layers are not indicated to be used. Since these ones likely represents particular patterns that will not help the new dataset. So, it is common to use the first layers in the transfer learning or fine tuning and add new fully connected ones in order to be trained from scratch."
      ]
    },
    {
      "metadata": {
        "id": "tEnMcEolXcsA",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def get_vgg16(input_images, cut_layer = \"pool5\", scope_name = \"vgg16\", fine_tunning = False):  \n",
        " \n",
        "    file_name = 'vgg16-20160129.tfmodel'\n",
        "    main_directory = \"./models/\"\n",
        "\n",
        "    vgg_path = os.path.join(main_directory,file_name)\n",
        "    if not os.path.exists(vgg_path):\n",
        "        vgg16_url = \"https://media.githubusercontent.com/media/pavelgonchar/colornet/master/vgg/tensorflow-vgg16/vgg16-20160129.tfmodel\"\n",
        "        utils.maybe_download_and_extract(vgg16_url, main_directory, file_name, file_name)\n",
        "\n",
        "\n",
        "    with open(vgg_path, mode='rb') as f:\n",
        "        content = f.read()\n",
        "        graph_def = tf.GraphDef()\n",
        "        graph_def.ParseFromString(content)\n",
        "        graph_def = tf.graph_util.extract_sub_graph(graph_def, [\"images\", cut_layer])\n",
        "        tf.import_graph_def(graph_def, input_map={\"images\": input_images})\n",
        "    del content\n",
        "\n",
        "    graph = tf.get_default_graph()\n",
        "    vgg_node = \"import/{}:0\".format(cut_layer) #It is possible to cut the graph in other node. \n",
        "                                               #For this, it is necessary to see the name of all layers by using the method \n",
        "                                               #\"get_operations()\": \"print(graph.get_operations())\" \n",
        "\n",
        "\n",
        "    vgg_trained_model = graph.get_tensor_by_name(\"{}/{}\".format(scope_name, vgg_node) )\n",
        "\n",
        "    if not fine_tunning:\n",
        "      print(\"Stopping gradient\")\n",
        "      vgg_trained_model = tf.stop_gradient(vgg_trained_model) #Just use it in case of transfer learning without fine tunning\n",
        "\n",
        "\n",
        "  #   print(graph.get_operations())\n",
        "    return vgg_trained_model, graph\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RpqQZnC8MVlL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Creating the model\n",
        "The function  \"transfer_learning_model\" is responsible for creating the model that will be used for recognizing the CIFAR10 images.\n",
        "\n",
        "The first scope (\"placeholders_variables\") defines:\n",
        "* ** input images** -  the images that will feed the model\n",
        "* **labels** - each image that feeds the input placeholder,  need to have a correspondent label, wich will feed this  placeholder when the loss was calculated.\n",
        "* **dropout_keep** - it defines a percent of neurons that will not be activated in each fully connected layer. The number to be fed is between 0 and 1.\n",
        "* **global_step** - As the train process is running, this variable stores the value of the current step. This value can be used for saving a checkpoint in an specific step, and, when restored, all the model continues the training process from this point/step.\n",
        "* **learning rate** - it defines the learning rate to be used by the optimizer. In this case, the global step is used in order to provide a decay point even whether the training is restarted or not. It starts from an initial learning rate and decays according to an specific rate, with each number of steps.\n",
        "\n",
        "These parameters are able to  influence directly the success of the training, so they are defined as hyperparameters of the model (class \"Hyperparameters\"), and must be treated and chosen carefully.\n"
      ]
    },
    {
      "metadata": {
        "id": "EjFZLWmlMWgr",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def transfer_learning_model(params = None, fine_tunning = False, bottleneck = False):\n",
        "   \n",
        "    if params is None:\n",
        "       params = Hyperparameters()\n",
        "      \n",
        "    with tf.name_scope('placeholders_variables'):\n",
        "        input_images = tf.placeholder(tf.float32, shape=[None,params.image_size, params.image_size, params.image_channels], name='input')\n",
        "        labels = tf.placeholder(tf.float32, shape=[None, params.num_classes], name='labels')\n",
        "        dropout_keep  =  tf.placeholder(tf.float32, name='dropout_keep')\n",
        "        global_step = tf.train.get_or_create_global_step()\n",
        "        learning_rate = tf.train.exponential_decay(params.initial_learning_rate, global_step, \n",
        "                                               params.decay_steps,params.decay_rate, staircase=True)       \n",
        "    \n",
        "\n",
        "    with tf.name_scope('vgg16'):\n",
        "       # Create a VGG16 model and reuse its weights.\n",
        "        vgg16_out,_ = get_vgg16(input_images=input_images,cut_layer = params.cut_layer, fine_tunning = fine_tunning)\n",
        "\n",
        "    with tf.name_scope(\"flatten\"):\n",
        "        flatten = tf.layers.flatten(vgg16_out, name=\"flatten\")\n",
        "    \n",
        "    if (not fine_tunning) and bottleneck:\n",
        "        out_list = flatten.shape.as_list()\n",
        "        BOTTLENECK_TENSOR_SIZE = np.prod(out_list[1:]) # All input layer size, less the batch size\n",
        "        with tf.name_scope('bottleneck'):\n",
        "            bottleneck_tensor = flatten\n",
        "            bottleneck_input = tf.placeholder(tf.float32,\n",
        "            shape=[None, BOTTLENECK_TENSOR_SIZE],\n",
        "            name='InputPlaceholder')\n",
        "\n",
        "        with tf.name_scope('fully_conn'):\n",
        "             logits = fc_model(bottleneck_input, params.hidden_layers) #Create a fully connected model that will be fed by the bottleneck\n",
        "    \n",
        "    else:\n",
        "        with tf.name_scope('fully_conn'):\n",
        "             logits = fc_model(flatten, params.hidden_layers) #Create a fully connected model that will be fed by the vgg16\n",
        "\n",
        "        \n",
        "\n",
        "    with tf.name_scope('loss'):\n",
        "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=labels))\n",
        "#         loss = regularize(loss)\n",
        "        tf.summary.scalar(\"loss\", loss)\n",
        "\n",
        "\n",
        "    with tf.name_scope('sgd'):\n",
        "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "        with tf.control_dependencies(update_ops):\n",
        "            optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
        "\n",
        "    with tf.name_scope('train_accuracy'):\n",
        "        acc = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
        "        acc = tf.reduce_mean(tf.cast(acc, tf.float32))\n",
        "        tf.summary.scalar(\"accuracy\", acc)\n",
        "   \n",
        "    \n",
        "    predictions = {\n",
        "                   \"classes\": tf.argmax(logits, 1),\n",
        "                   \"probs\" :  tf.nn.softmax(logits), \n",
        "                   \"labels\": tf.argmax(labels, 1)\n",
        "                   }\n",
        "    model = {\n",
        "              \"global_step\": global_step,\n",
        "              \"images\": input_images,\n",
        "              \"labels\": labels,    \n",
        "              \"loss\" : loss,\n",
        "              \"optimizer\": optimizer,\n",
        "              \"accuracy\": acc,\n",
        "              \"predictions\":predictions,\n",
        "              \"keep\": dropout_keep\n",
        "          }\n",
        "\n",
        "    \n",
        "    if (not fine_tunning) and bottleneck:\n",
        "        model.update({\"bottleneck_tensor\":bottleneck_tensor})\n",
        "        model.update({\"bottleneck_input\":bottleneck_input})\n",
        "          \n",
        "    return model\n",
        "        \n",
        "def get_fc_weights(w_inputs, w_output, id=0):\n",
        "    weight= tf.Variable(tf.truncated_normal([w_inputs, w_output]), name=\"{}/weight\".format(id))\n",
        "    bias =  tf.Variable(tf.truncated_normal([w_output]), name=\"{}/bias\".format(id))\n",
        "    return weight, bias  \n",
        "\n",
        "def logits_layer(fc_layer, n_classes):\n",
        "    out_shape = fc_layer.shape.as_list()\n",
        "    w, b = get_fc_weights(np.prod(out_shape[1:]), n_classes, \"logits/weight\")\n",
        "    logits = tf.add(tf.matmul(fc_layer, w), b, name=\"logits\")\n",
        "    return logits\n",
        "      \n",
        "def fc_layer(input_layer, number_of_units, keep = None, layer_id = \"fc\"):\n",
        "    pl_list = input_layer.shape.as_list()\n",
        "    input_size = np.prod(pl_list[1:])\n",
        "\n",
        "    w, b = get_fc_weights(input_size, number_of_units, layer_id)  \n",
        "    fc_layer = tf.matmul(input_layer, w, name=\"{}/matmul\".format(layer_id))\n",
        "    fc_layer = tf.nn.bias_add(fc_layer, b, name=\"{}/bias-add\".format(layer_id))\n",
        "\n",
        "    if keep is not None:\n",
        "      fc_layer = tf.nn.dropout(fc_layer, keep, name=\"{}/dropout\".format(layer_id))\n",
        "    else:\n",
        "      print(\"Dropout was disabled.\")\n",
        "\n",
        "    fc_layer = tf.nn.relu(fc_layer, name=\"{}/relu\".format(layer_id))\n",
        "    return fc_layer\n",
        "      \n",
        "def regularize(loss, type = 1, scale = 0.005, scope = None):\n",
        "    if type == 1:\n",
        "        regularizer = tf.contrib.layers.l1_regularizer( scale=scale, scope=scope)\n",
        "    else:\n",
        "        regularizer = tf.contrib.layers.l2_regularizer( scale=scale, scope=scope)\n",
        "\n",
        "    weights = tf.trainable_variables() # all vars of your graph\n",
        "    regularization_penalty = tf.contrib.layers.apply_regularization(regularizer, weights)\n",
        "    regularized_loss = loss + regularization_penalty\n",
        "    return regularized_loss\n",
        "\n",
        "def fc_model(flatten, hidden_layers = [512], keep = None):\n",
        "    fc = flatten\n",
        "    id = 1\n",
        "    for num_neurons in hidden_layers:\n",
        "      fc = fc_layer(fc, num_neurons, keep,  \"fc{}\".format(id) )\n",
        "      id = id+1\n",
        "\n",
        "    logits = logits_layer(fc, params.num_classes)\n",
        "    return logits\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d8MnlJ4QNrFf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Creating a session\n",
        "The function \"create_monitored_session\" creates a tensorflow session able to restore weights and/or save them. The parameter \"checkpoint_dir\" represents where the weights were saved or where one wants to save them. All the save/restore process is performed automatically by tensorflow.\n",
        "\n",
        "As default, tensorflow allocates all GPU memory in the first called to the session run, thus the \"tf.ConfigProto()\", by setting the \"True\" to the \"gpu_options.allow_growth\", allows the gradual increasing of memory. In other words, it allows to allocate the GPU memory by demanding. This is  important mainly when more than one training or prediction process is running on the same GPU.\n"
      ]
    },
    {
      "metadata": {
        "id": "ldO41ReMNrXJ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def create_monitored_session(model,iter_per_epoch, checkpoint_dir):\n",
        "    config = tf.ConfigProto()\n",
        "    config.gpu_options.allow_growth = True\n",
        "\n",
        "\n",
        "    sess = tf.train.MonitoredTrainingSession(checkpoint_dir=checkpoint_dir,\n",
        "                                        save_checkpoint_secs=120,\n",
        "                                        log_step_count_steps=iter_per_epoch,\n",
        "                                        save_summaries_steps=iter_per_epoch,\n",
        "                                        config=config) \n",
        "    return sess"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EaAGt5L8O--z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Testing the model\n",
        "The function \"test\" is responsible for applying the test dataset through the trained model. Thus, it is possible to monitor the model progress. This function could be change in order to do a validation test, which uses the validation dataset,  rather than just a test. It would be helpful for problems that do not release a labeled test dataset."
      ]
    },
    {
      "metadata": {
        "id": "hBjCG-7ALuas",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def test(sess, model,input_data_placeholder, data, labels, batch_size = 128):\n",
        "    global_accuracy = 0\n",
        "    predictions = {\n",
        "                   \"classes\":[],\n",
        "                   \"probs\":[],\n",
        "                   \"labels\":[]\n",
        "                  }\n",
        "\n",
        "    size = len(data)//batch_size\n",
        "    indices = list(range(len(data)))\n",
        "\n",
        "    for i in range(size+1):\n",
        "\n",
        "        begin = i*batch_size\n",
        "        end = (i+1)*batch_size\n",
        "        end = len(data) if end >= len(data) else end\n",
        "\n",
        "        next_bach_indices = indices[begin:end]\n",
        "        batch_xs = data[next_bach_indices]\n",
        "        batch_ys = labels[next_bach_indices]\n",
        "\n",
        "        pred = sess.run(model[\"predictions\"],\n",
        "            feed_dict={input_data_placeholder: batch_xs, model[\"labels\"]: batch_ys, model[\"keep\"]:1.0})\n",
        "\n",
        "        predictions[\"classes\"].extend(pred[\"classes\"])\n",
        "        predictions[\"probs\"].extend(pred[\"probs\"])\n",
        "        predictions[\"labels\"].extend(pred[\"labels\"])\n",
        "\n",
        "\n",
        "    correct = list (map(lambda x,y: 1 if x==y else 0, predictions[\"labels\"] , predictions[\"classes\"]))\n",
        "    acc = np.mean(correct ) *100\n",
        "\n",
        "    mes = \"--> Test accuracy: {:.2f}% ({}/{})\"\n",
        "    print(mes.format( acc, sum(correct), len(data)))\n",
        "\n",
        "    return predictions\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CXZQdFvFOEwx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Training the model: the mainly function\n",
        "The \"train\" function is responsible for training the model. It starts checking the hyperparameters and resetting the default graph.  Then, the dataset is loaded by using the class \"util\". The next step consists of  creating the model, where the tensorflow graph is created. Now, a monitored season is created too. This kind of session will save and restore the model automatically, which will be very important when an unexpected event occurs and the model stop the training (such as a power outage or when the Google Colab finishes the session during the training).\n",
        "\n",
        "With the model and the session created, you are able, if you want, to generate or load the bottlenecks files. This is what the next lines are doing. One of the most important results of theses lines is  obtaining the tensor \"input_data_placeholder\". It is important because when the bottleneck option is chosen, the \"feed_dict\" must feed the placeholder of the \"bottleneck\" rather than the one that feeds the VVG16 inputs. Thus, if the bottleneck is chosen, the input placeholder will be the \"model[bottleneack_input]\", else, it will be the input tensor of the vgg16, \"model[images]\".\n",
        "\n",
        "In the  the beginning of each epoch, in order to ensure the randomness of the baths,  a list containing the dataset indices is shuffled. So, at every batch, a new range of indices is taken and the batch may feed the placeholder.\n",
        "\n",
        "Therefore, the session can call the optimizer and train the model. Finally, the last two steps consiste of calling the test funtion for checking the training result every epoch, and generate a confusion matrix with the result of the last one.\n"
      ]
    },
    {
      "metadata": {
        "id": "SoXSuCQ3OFWN",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def train(params = None):\n",
        "    if params is None:\n",
        "      params = Hyperparameters()\n",
        "      \n",
        "    tf.reset_default_graph()\n",
        "\n",
        "    train_data, train_labels = utils.get_data_set(\"train\")\n",
        "    train_data, train_labels = utils.data_augmentation(train_data, train_labels)\n",
        "    \n",
        "    test_data, test_labels = utils.get_data_set(\"test\")  \n",
        "    \n",
        "    model = transfer_learning_model(params, params.fine_tunning, params.bottleneck)\n",
        "   \n",
        "    steps_per_epoch = int(math.ceil(len(train_data) /  params.batch_size))\n",
        "    sess = create_monitored_session(model,steps_per_epoch, params.check_points_path)\n",
        "    \n",
        "    \n",
        "    if (not params.fine_tunning) and params.bottleneck:\n",
        "        indices = list( range(len(train_data)) )\n",
        "        shuffle(indices)\n",
        "        \n",
        "        shuffled_data = train_data[indices]\n",
        "        shuffled_labels = train_labels[indices]\n",
        "        \n",
        "        bottleneck_train = utils.get_or_generate_bottleneck(sess, model, \"bottleneck_vgg16_{}_train\".format(params.cut_layer), shuffled_data, shuffled_labels)\n",
        "        bottleneck_test = utils.get_or_generate_bottleneck(sess, model, \"bottleneck_vgg16_{}_test\".format(params.cut_layer), test_data, test_labels)\n",
        "        \n",
        "        train_data, train_labels  = bottleneck_train[\"data\"], bottleneck_train[\"labels\"]\n",
        "        test_data, test_labels = bottleneck_test[\"data\"], bottleneck_test[\"labels\"]\n",
        "        del bottleneck_train, bottleneck_test\n",
        "        \n",
        "        input_data_placeholder = model[\"bottleneck_input\"]\n",
        "        \n",
        "    else:\n",
        "        input_data_placeholder = model[\"images\"]\n",
        "        \n",
        "        \n",
        "    \n",
        "    indices = list( range(len(train_data)) )\n",
        "    msg = \"--> Global step: {:>5} - Last batch acc: {:.2f}% - Batch_loss: {:.4f} - ({:.2f}, {:.2f}) (steps,images)/sec\"\n",
        "    \n",
        "    for epoch in range(params.num_epochs):\n",
        "        start_time = time()\n",
        "        \n",
        "        print(\"\\n*************************************************************\")\n",
        "        print(\"Epoch {}/{}\".format(epoch+1,params.num_epochs))\n",
        "        \n",
        "        shuffle(indices)  \n",
        "        for s in range(steps_per_epoch):\n",
        "          \n",
        "            indices_next_batch = indices[s *  params.batch_size : (s+1) * params.batch_size]\n",
        "            batch_data = train_data[indices_next_batch]\n",
        "            batch_labels = train_labels[indices_next_batch]\n",
        "            \n",
        "            _, batch_loss, batch_acc,step = sess.run(\n",
        "                [model[\"optimizer\"], model[\"loss\"], model[\"accuracy\"], model[\"global_step\"],],\n",
        "                feed_dict={input_data_placeholder: batch_data, model[\"labels\"]: batch_labels, model[\"keep\"]:params.keep})\n",
        "        \n",
        "        duration = time() - start_time\n",
        "\n",
        "        print(msg.format(step,  batch_acc*100, batch_loss, (steps_per_epoch / duration), (steps_per_epoch*params.batch_size / duration) ))\n",
        "\n",
        "        \n",
        "        _ = test(sess, model, input_data_placeholder, test_data, test_labels )\n",
        "    \n",
        "    predictions = test(sess, model, input_data_placeholder, test_data, test_labels )\n",
        "\n",
        "    sess.close()\n",
        "    \n",
        "    class_names = [\"airplane\",\"automobile\",\"bird\",\"cat\",\"deer\",\"dog\",\"frog\",\"horse\",\"ship\",\"truck\"] \n",
        "    utils.generate_confusion_matrix(predictions, class_names)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YN2o7cb8OSgp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This part of code instantiates a \"Hyperparameters\" class, changes it and passes it as parameter to the train function. Thus, the training can be started."
      ]
    },
    {
      "metadata": {
        "id": "7_0NbT5ROSzE",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  params = Hyperparameters()\n",
        "  params.num_epochs = 100\n",
        "  params.hidden_layers = [1024]\n",
        "  params.initial_learning_rate = 1e-3\n",
        "  params.cut_layer = \"pool4\"\n",
        "\n",
        "  train(params)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Iel8lO7CGtzy",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
