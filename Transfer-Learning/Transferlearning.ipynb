{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transferlearning.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "qMV758B9en_F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Transfer Learning (Tensorflow +  vgg16 + CIFAR10 )\n",
        "\n",
        "\n",
        "In this tutorial you will see a complete guide of how to apply deep learning on image recognition by studying an example of how to transfer learning from the VGG16 model (trained with IMAGENET dataset) to solve the CIFAR10 problem. After follow the next steps, it will be easy to adapt the code in order to apply the vgg16 model for recognizing others image classification problems.\n",
        "\n",
        "\n",
        "###Requirements:\n",
        "- Personal \n",
        "  - Python (basic) <br>\n",
        "  - Tensorflow (basic) <br>\n",
        "  - Convolutional Neural Network - CNN (basic) <br>\n",
        "\n",
        "- Software:\n",
        "  - Python 3.6 or later <br>\n",
        "  - Tensorflow 1.6 or later<br>\n",
        "\n",
        "- Hardware \n",
        "  - This code was developed to be run on <a href=\"https://colab.research.google.com/notebooks/welcome.ipynb\">Google Colab</a>. But, taking off the Google colab dependecies, it can be run on a computer with more than 8 GB of RAM. A good processor and/or GPU will do the training faster, but it is not mandatory.\n",
        "\n",
        "##A little of theory\n",
        "- ** What is Transfer Learning?**<br>\n",
        "\n",
        "  One of the most important characteristics of the deep learning is the training time. Many models could take hours, days, or even weeks to get trained. Thus, in order to accelerate this process, powerful machines are often used. But not all researchers have a machine with this power of processing. What lead small groups to not be able to use this models for solving their problems. However, great companies or research groups, such as Google and Facebook, have been training the models and releasing them for the community.\n",
        "  \n",
        " Many works in deep learning field say that the inicialize of the weights is one of the most important tasks when one wishes to train a deep model. Therefore, the reuse the weights of models with similar tasks is a common practice. That reuse of weight is called transfer learning, because the knowledge acquired during the training is now transferred to other problem. Lets give you an example of transfer learning: imagine 2 persons who want to learn to play piano. The first one never plays any instrument, than his/her knowledge needs to start from scratch, what can be hard, considering that the piano is a difficult instrument to play. On the other hands, the other one already plays guitar, so his/her knowledge about music, rhythm and chords can be reused to get easier the process of learning to play piano. Who of those persons will learning faster? Of course the second one, because the possibility of  transferring his/her knowledge about to play guitar.\n",
        "\n",
        " Thus, in order to accelerate the time of training, its common to use trained model to solve different tasks.\n",
        "In the majority of cases, transfer learning is able to reach good results, so is important to now how it works and the better way to use it.\n",
        "\n",
        " It is important to mention that the transfer learning do not just reuse the weights, it is possible reuse all the model, what can became easier the process of applying deep models to solve problems.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "- **When to use transfer learning?**<br>\n",
        "\n",
        "  This is a good question. Many people who uses this technique do not know how to answer to this question. Because they just follow some tutorials and do not learn more about the theory behind it.\n",
        "As mentioned by <a href=\"http://cs231n.github.io/transfer-learning/\"> Fei-Fei Li </a>, the transfer learning can occur in 4 different contexts, but here we summarize them in only 2 points, such as:\n",
        " \n",
        "  1. When the new task is similar to the trained model and de dataset is enough big, the transfer learning must be made. In this case, only the fully connected layer need to be trained/retrained. The CNN is taken only as a feature extractor, and should not be trained as well.\n",
        "  \n",
        " 2. When the task is different of the original or the dataset is small, it is necessary to do a fine tuning. This technique consists of doing adjusts in the CNN weights while the fully connected layers are training.\n",
        "\n",
        " Do not worry if you do not know how to use these techniques yet, this tutorial will cover both transfer learning and fine tuning. But, if you want to see a complete guide about this and others related topics, please, see this  course from Stanford University, titled: <a href=\"http://cs231n.github.io/\">Convolutional Neural Networks for Visual Recognition. </a>\n",
        "\n",
        " \n",
        "- **What is a Bottleneck Approach?**\n",
        "\n",
        "  When the first case occurs, the CNN layers continue without change during all the training. What could cause a waste of processing because an image that pass many times through the CNN has the same result in its output. So, in order to get faster the training process, a technique called \"bottleneck\" is applied. It consists of passing all the training dataset through the CNN and storing its results on disk. Therefore, the first fully connected layer will receive in the next steps the data already processed, what can save a considerable time depending on the machine and the length of the CNN. It is not common to find tutorials that address this subject. But, due its importance for decrease of training time, this is addressed here.\n",
        "\n",
        "\n",
        "- **VGG16 Architecture**\n",
        "\n",
        " The VGG16 is one of the models that got the top accuracy in the IMAGENET challenge in 2014, achiving 92.7% top-5 test accuracy. This challenge consists of classifying more than 100,000 images in 1,000 differents classes. For it, there are more than 1,2 million images for training and 50.000 for validation.\n",
        " \n",
        " The vgg16 is one of the most used model for applying transfer learning in image recognition problems. Furthermore, it has a traditional deep architecture, which is just based on convolutional and pooling layers. The Figure 1 shows a representation of it. Notice that it is composed by 16 principal layers, where 13 are convolutional, 1 is flatten, 1 is fully connected and the last one is a softmax classifier. There are 5 more layers corresponding to the max pooling operation, but these layers was not taken in account for giving the model name.\n",
        "\n",
        " <center>\n",
        " <img src=\"http://www.cs.toronto.edu/~frossard/post/vgg16/vgg16.png\" width=\"480\" height=\"240\" align=\"center\"/><br>\n",
        "  <b> Figure 1: The VGG16 architecture</b></center> <br><br>\n",
        "                                   \n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "- **The CIFAR10 dataset**\n",
        "The CIFAR10 is a dataset with $60.000$ tiny images ($50.000$ for training and $10.000$ for test). Each image has a shape of $37 x 37 x 3$. All the images are distributed equaly between $10$ classes (airplane, automobile, bird, cat, deer, dog, frog, horse, ship and truck), as shown by the Figure 2.\n",
        " \n",
        "<table>\n",
        "    <tbody><tr>\n",
        "        <td class=\"cifar-class-name\">airplane</td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/airplane1.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/airplane2.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/airplane3.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/airplane4.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/airplane5.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/airplane6.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/airplane7.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/airplane8.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/airplane9.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/airplane10.png\" class=\"cifar-sample\"></td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td class=\"cifar-class-name\">automobile</td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/automobile1.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/automobile2.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/automobile3.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/automobile4.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/automobile5.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/automobile6.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/automobile7.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/automobile8.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/automobile9.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/automobile10.png\" class=\"cifar-sample\"></td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td class=\"cifar-class-name\">bird</td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/bird1.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/bird2.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/bird3.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/bird4.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/bird5.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/bird6.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/bird7.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/bird8.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/bird9.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/bird10.png\" class=\"cifar-sample\"></td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td class=\"cifar-class-name\">cat</td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/cat1.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/cat2.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/cat3.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/cat4.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/cat5.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/cat6.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/cat7.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/cat8.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/cat9.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/cat10.png\" class=\"cifar-sample\"></td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td class=\"cifar-class-name\">deer</td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/deer1.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/deer2.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/deer3.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/deer4.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/deer5.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/deer6.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/deer7.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/deer8.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/deer9.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/deer10.png\" class=\"cifar-sample\"></td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td class=\"cifar-class-name\">dog</td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/dog1.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/dog2.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/dog3.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/dog4.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/dog5.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/dog6.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/dog7.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/dog8.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/dog9.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/dog10.png\" class=\"cifar-sample\"></td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td class=\"cifar-class-name\">frog</td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/frog1.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/frog2.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/frog3.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/frog4.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/frog5.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/frog6.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/frog7.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/frog8.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/frog9.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/frog10.png\" class=\"cifar-sample\"></td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td class=\"cifar-class-name\">horse</td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/horse1.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/horse2.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/horse3.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/horse4.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/horse5.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/horse6.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/horse7.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/horse8.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/horse9.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/horse10.png\" class=\"cifar-sample\"></td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td class=\"cifar-class-name\">ship</td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/ship1.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/ship2.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/ship3.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/ship4.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/ship5.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/ship6.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/ship7.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/ship8.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/ship9.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/ship10.png\" class=\"cifar-sample\"></td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td class=\"cifar-class-name\">truck</td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/truck1.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/truck2.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/truck3.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/truck4.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/truck5.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/truck6.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/truck7.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/truck8.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/truck9.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/truck10.png\" class=\"cifar-sample\"></td>\n",
        "    </tr>\n",
        "       \n",
        "</tbody>\n",
        "\n",
        "</table>\n",
        "<center><b>Figure 2: A sample of CIFAR 10 dataset</b></center><br><br>\n",
        "\n",
        "\n",
        "Notice that the VGG16 was trained receiving an input image of $224 x 224 x 3$ and  the CIFAR10 has $32x32x3$ instead. The vgg16 was trained to recognize images in 1,000 different classes, but CIFAR10 has only 10. Thus, the new dataset is different from the original one, what can lead us to guess that a fine tunning is the best solution for this problem. \n",
        " \n",
        " Despite this guess, we will show you how to do the transfer learning without fine tunning, and how to use the bottleneck when necessary.\n"
      ]
    },
    {
      "metadata": {
        "id": "mqcjjpnB-AMs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Lets get started with the code\n",
        "The code below performs a complete task of transfer learning. All of it was made thinking in an easy way to learn this subject and a easy way of modifying it in order to resolve other sorts of tasks.\n"
      ]
    },
    {
      "metadata": {
        "id": "E3v8U2KBDmby",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###All the necessary imports\n",
        "Note that this code was made for running on Google Colab. Then, its usage outside this plataform requires adaptations. As taking off all the Google Colab dependencies and download manually the VGG16 model and put it into the folder \"./model\". The model can be downloaded <a href=\"https://github.com/ry/tensorflow-vgg16/blob/master/vgg16-20160129.tfmodel.torrent\">here</a>:"
      ]
    },
    {
      "metadata": {
        "id": "AmDLc8t7WWL6",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        },
        "cellView": "both"
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline  \n",
        "import pickle\n",
        "import numpy as np\n",
        "import os\n",
        "from urllib.request import urlretrieve\n",
        "import tarfile\n",
        "import zipfile\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from time import time\n",
        "import skimage as sk\n",
        "from skimage import transform\n",
        "from skimage import util\n",
        "import random\n",
        "import math\n",
        "import os.path\n",
        "from random import shuffle\n",
        "import logging\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from google.colab import files\n",
        "from itertools import product\n",
        "!pip install googledrivedownloader\n",
        "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JgYef7Lv-Ljr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### Class that defines the principals hyperparameters used by the model\n"
      ]
    },
    {
      "metadata": {
        "id": "uIuAiX69WXn8",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class Hyperparameters:\n",
        "  def __init__(self):\n",
        "    self.image_size = 32\n",
        "    self.image_channels = 3\n",
        "    self.num_classes = 10\n",
        "    self.initial_learning_rate = 1e-4\n",
        "    self.decay_steps = 1e3\n",
        "    self.decay_rate = 0.98\n",
        "    self.cut_layer = \"pool5\"\n",
        "    self.hidden_layers = [512]\n",
        "    self.batch_size = 128\n",
        "    self.num_epochs = 200\n",
        "    self.check_points_path= \"./tensorboard/cifar10_vgg16\"\n",
        "    self.keep = 1.0\n",
        "    self.fine_tunning = False\n",
        "    self.bottleneck = True\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8sB2nJT3-mli",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Class that  provides same utilities for the model, such as downloads files,  gets dataset, does  data augmentation,  generates bottlenecks files and creates a confusion matrix from the model."
      ]
    },
    {
      "metadata": {
        "id": "lqSlsfGaug9m",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class utils:\n",
        "      def get_or_generate_bottleneck( sess, model, file_name, dataset, labels, batch_size = 128):\n",
        "\n",
        "          path_file = os.path.join(\"./data_set\",file_name+\".pkl\")\n",
        "          if(os.path.exists(path_file)):\n",
        "                print(\"Loading bottleneck from \\\"{}\\\" \".format(path_file))\n",
        "                with open(path_file, 'rb') as f:\n",
        "                   return pickle.load(f)\n",
        "\n",
        "          bottleneck_data = []\n",
        "          original_labels = []\n",
        "\n",
        "          print(\"Generating Bottleneck \\\"{}.pkl\\\" \".format(file_name) )\n",
        "          count = 0\n",
        "          amount = len(labels) // batch_size\n",
        "          indices = list(range(len(labels)))\n",
        "          for i in range(amount+1):\n",
        "\n",
        "                if (i+1)*batch_size < len(indices):\n",
        "                  indices_next_batch = indices[i*batch_size: (i+1)*batch_size]\n",
        "                else:\n",
        "                   indices_next_batch = indices[i*batch_size:]\n",
        "                batch_size = len(indices_next_batch)\n",
        "\n",
        "                data = dataset[indices_next_batch]\n",
        "                label = labels[indices_next_batch]\n",
        "                input_size = np.prod(model[\"bottleneck_tensor\"].shape.as_list()[1:])\n",
        "                tensor = sess.run(model[\"bottleneck_tensor\"], feed_dict={model[\"images\"]:data, model[\"bottleneck_input\"]:np.zeros((batch_size,input_size)), model[\"labels\"]:label,model[\"keep\"]:1.0})\n",
        "                for t in range(batch_size):\n",
        "                  bottleneck_data.append(np.squeeze(tensor[t]))\n",
        "                  original_labels.append(np.squeeze(label[t]))\n",
        "          \n",
        "          bottleneck = {\n",
        "              \"data\":np.array(bottleneck_data),\n",
        "              \"labels\":np.array(original_labels)\n",
        "          } \n",
        "          \n",
        "          with open(path_file, 'wb') as f:\n",
        "            pickle.dump(bottleneck, f)\n",
        "\n",
        "\n",
        "          print(\"Done\")   \n",
        "\n",
        "          return bottleneck\n",
        "\n",
        "\n",
        "\n",
        "      def get_data_set(name=\"train\"):\n",
        "          x = None\n",
        "          y = None\n",
        "          folder_name = 'cifar_10'\n",
        "          main_directory = \"./data_set\"\n",
        "          url = \"http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
        "\n",
        "          utils.maybe_download_and_extract(url, main_directory,folder_name, \"cifar-10-batches-py\")\n",
        "\n",
        "\n",
        "          f = open(os.path.join(main_directory,folder_name,\"batches.meta\"), 'rb')\n",
        "          f.close()\n",
        "\n",
        "          if name is \"train\":\n",
        "              for i in range(5):\n",
        "                  f = open('./data_set/'+folder_name+'/data_batch_' + str(i + 1), 'rb')\n",
        "                  datadict = pickle.load(f, encoding='latin1')\n",
        "                  f.close()\n",
        "\n",
        "                  _X = datadict[\"data\"]\n",
        "                  _Y = datadict['labels']\n",
        "\n",
        "                  _X = np.array(_X, dtype=float) / 255.0\n",
        "                  _X = _X.reshape([-1, 3, 32, 32])\n",
        "                  _X = _X.transpose([0, 2, 3, 1])\n",
        "#                   _X = _X.reshape(-1, 32*32*3)\n",
        "\n",
        "                  if x is None:\n",
        "                      x = _X\n",
        "                      y = _Y\n",
        "                  else:\n",
        "                      x = np.concatenate((x, _X), axis=0)\n",
        "                      y = np.concatenate((y, _Y), axis=0)\n",
        "\n",
        "          elif name is \"test\":\n",
        "              f = open('./data_set/'+folder_name+'/test_batch', 'rb')\n",
        "              datadict = pickle.load(f, encoding='latin1')\n",
        "              f.close()\n",
        "\n",
        "              x = datadict[\"data\"]\n",
        "              y = np.array(datadict['labels'])\n",
        "\n",
        "              x = np.array(x, dtype=float) / 255.0\n",
        "              x = x.reshape([-1, 3, 32, 32])\n",
        "              x = x.transpose([0, 2, 3, 1])\n",
        "#               x = x.reshape(-1, 32*32*3)\n",
        "\n",
        "          return x, utils._dense_to_one_hot(y)\n",
        "\n",
        "\n",
        "      def _dense_to_one_hot( labels_dense, num_classes=10):\n",
        "          num_labels = labels_dense.shape[0]\n",
        "          index_offset = np.arange(num_labels) * num_classes\n",
        "          labels_one_hot = np.zeros((num_labels, num_classes))\n",
        "          labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n",
        "\n",
        "          return labels_one_hot\n",
        "\n",
        "\n",
        "      \n",
        "\n",
        "\n",
        "      def maybe_download_and_extract( url, main_directory,filename, original_name):\n",
        "          def _print_download_progress( count, block_size, total_size):\n",
        "            pct_complete = float(count * block_size) / total_size\n",
        "            msg = \"\\r --> progress: {0:.1%}\".format(pct_complete)\n",
        "            sys.stdout.write(msg)\n",
        "            sys.stdout.flush()\n",
        "          \n",
        "          if not os.path.exists(main_directory):\n",
        "              os.makedirs(main_directory)\n",
        "              url_file_name = url.split('/')[-1]\n",
        "              zip_file = os.path.join(main_directory,url_file_name)\n",
        "              print(\"Downloading \",url_file_name)\n",
        "\n",
        "              try:\n",
        "                file_path, _ = urlretrieve(url=url, filename= zip_file, reporthook=_print_download_progress)\n",
        "              except:\n",
        "                os.system(\"rm -r \"+main_directory)\n",
        "                print(\"An error occurred while downloading: \",url)\n",
        "\n",
        "                if(original_name == 'vgg16-20160129.tfmodel'):\n",
        "                  print(\"This could be for a problem with github. We will try downloading from the Google Drive\")\n",
        "                  from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "\n",
        "                  gdd.download_file_from_google_drive(file_id='1xJZDLu_TK_SyQz-SaetAL_VOFY7xdAt5',\n",
        "                                                      dest_path='./models/vgg16-20160129.tfmodel',\n",
        "                                                      unzip=False)\n",
        "                else: print(\"This could be for a problem with the storage site. Try again later\")\n",
        "                return\n",
        "\n",
        "              print(\"\\nDownload finished.\")\n",
        "              if file_path.endswith(\".zip\"):\n",
        "                  print( \"Extracting files.\")\n",
        "\n",
        "                  zipfile.ZipFile(file=file_path, mode=\"r\").extractall(main_directory)\n",
        "              elif file_path.endswith((\".tar.gz\", \".tgz\")):\n",
        "                  print( \"Extracting files.\")\n",
        "                  tarfile.open(name=file_path, mode=\"r:gz\").extractall(main_directory)\n",
        "                  os.remove(file_path)\n",
        "\n",
        "              os.rename(os.path.join(main_directory,original_name), os.path.join(main_directory,filename))\n",
        "              print(\"Done.\")\n",
        "     \n",
        "      def data_augmentation(images, labels):\n",
        "        \n",
        "          def random_rotation(image_array):\n",
        "              # pick a random degree of rotation between 25% on the left and 25% on the right\n",
        "              random_degree = random.uniform(-15, 15)\n",
        "              return sk.transform.rotate(image_array, random_degree)\n",
        "\n",
        "          def random_noise(image_array):\n",
        "              # add random noise to the image\n",
        "              return sk.util.random_noise(image_array)\n",
        "\n",
        "          def horizontal_flip(image_array):\n",
        "              # horizontal flip doesn't need skimage, it's easy as flipping the image array of pixels !\n",
        "              return image_array[:, ::-1]\n",
        "          print(\"Augmenting data...\")\n",
        "          aug_images = []\n",
        "          aug_labels = []\n",
        "\n",
        "          aug_images.extend( list(map(random_rotation, images)) )\n",
        "          aug_labels.extend(labels)\n",
        "          aug_images.extend( list(map(random_noise,    images)) )\n",
        "          aug_labels.extend(labels)\n",
        "          aug_images.extend( list(map(horizontal_flip, images)) )\n",
        "          aug_labels.extend(labels)\n",
        "\n",
        "\n",
        "          return np.array(aug_images), np.array(aug_labels)\n",
        "        \n",
        "        \n",
        "        \n",
        "      \n",
        "      def generate_confusion_matrix( predictions, class_names):\n",
        "        \n",
        "        def plot_confusion_matrix(cm, classes,\n",
        "                                    normalize=False,\n",
        "                                    title='Confusion matrix',\n",
        "                                    cmap=plt.cm.Blues):\n",
        "                \"\"\"\n",
        "                This function prints and plots the confusion matrix.\n",
        "                Normalization can be applied by setting `normalize=True`.\n",
        "                \"\"\"\n",
        "                if normalize:\n",
        "                    cm = 100 * cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "                    print(\"Normalized confusion matrix\")\n",
        "                else:\n",
        "                    print('Confusion matrix, without normalization')\n",
        "\n",
        "                print(cm.shape)\n",
        "\n",
        "                plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "                plt.title(title)\n",
        "                plt.colorbar()\n",
        "                \n",
        "                tick_marks = np.arange(len(classes))\n",
        "               \n",
        "          \n",
        "                plt.xticks(tick_marks, classes, rotation=45)\n",
        "                plt.yticks(tick_marks, classes)\n",
        "\n",
        "                fmt = '.2f' if normalize else 'd'\n",
        "                thresh = cm.max() / 2.\n",
        "                symbol = \"%\" if normalize else \"\"\n",
        "                for i, j in product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "                    plt.text(j, i, format(cm[i, j], fmt)+symbol,\n",
        "                            horizontalalignment=\"center\",\n",
        "                            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "                plt.tight_layout()\n",
        "                plt.ylabel('Real')\n",
        "                plt.xlabel('Predicted')\n",
        "        # Compute confusion matrix\n",
        "        cnf_matrix = confusion_matrix(predictions[\"labels\"],predictions[\"classes\"])\n",
        "        np.set_printoptions(precision=2)\n",
        "        \n",
        "\n",
        "#         # Plot non-normalized confusion matrix\n",
        "#         plt.figure(figsize=(10,7))\n",
        "#         plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
        "#                             title='Confusion matrix, without normalization')\n",
        "#         plt.grid('off')\n",
        "\n",
        "        # # Plot normalized confusion matrix\n",
        "        plt.figure(figsize=(10,7))\n",
        "        plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
        "                            title='Normalized confusion matrix')\n",
        "        plt.grid('off')\n",
        "\n",
        "        #plt.savefig(\"./confusion_matrix.png\") #Save the confision matrix as a .png figure.\n",
        "        plt.show()\n",
        "#         "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "i14EUs_7_3ht",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###The function \"get_vgg16\" returns a pretrained vgg16 model.\n",
        "\n",
        "All the work of load and restore the weights of the model is responsibility of tensorflow. We just need to choose which layer we want to cut and pass it as parameter for the function \"get_vgg16\". In transfer learning it is common to dispose the fully connected layer and reuse only the convolutional ones. It occurs because the new problem/dataset use to be different from the original (such one that was used for training the model), and the numbers of classes is often different as well.<br>\n",
        "\n",
        "In a CNN, the first layers are responsible for selecting borders, the middle layers for selecting some kinds of patterns, based on combinations of those edges obtained previously and de last ones for composing patterns with a high level of representation, also known as semantic layers. Thereby, when the new dataset is much different of the original, the last layers are not indicated to be used. Since these ones likely represents particular patterns that will not help the new dataset. So, it is common to use the first layers in the transfer learning or fine tuning and add new fully connected ones in order to be trained from scratch."
      ]
    },
    {
      "metadata": {
        "id": "tEnMcEolXcsA",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def get_vgg16(input_images, cut_layer = \"pool5\", scope_name = \"vgg16\", fine_tunning = False):  \n",
        " \n",
        "  file_name = 'vgg16-20160129.tfmodel'\n",
        "  main_directory = \"./models/\"\n",
        "#   !rm -r ./models/\n",
        "  vgg_path = os.path.join(main_directory,file_name)\n",
        "  if not os.path.exists(vgg_path):\n",
        "      vgg16_url = \"https://media.githubusercontent.com/media/pavelgonchar/colornet/master/vgg/tensorflow-vgg16/vgg16-20160129.tfmodel\"\n",
        "      utils.maybe_download_and_extract(vgg16_url, main_directory, file_name, file_name)\n",
        "\n",
        "\n",
        "  with open(vgg_path, mode='rb') as f:\n",
        "      content = f.read()\n",
        "      graph_def = tf.GraphDef()\n",
        "      graph_def.ParseFromString(content)\n",
        "      graph_def = tf.graph_util.extract_sub_graph(graph_def, [\"images\", cut_layer])\n",
        "      tf.import_graph_def(graph_def, input_map={\"images\": input_images})\n",
        "  del content\n",
        "\n",
        "  graph = tf.get_default_graph()\n",
        "  vgg_node = \"import/{}:0\".format(cut_layer) #It possible to cut the graph in other node. \n",
        "                                             #For this, it is necessary to see the name of all layers by using the method \n",
        "                                             #\"get_operations()\": \"print(graph.get_operations())\" \n",
        "\n",
        "  \n",
        "  vgg_trained_model = graph.get_tensor_by_name(\"{}/{}\".format(scope_name, vgg_node) )\n",
        "  \n",
        "  if not fine_tunning:\n",
        "    print(\"Stopping gradient\")\n",
        "    vgg_trained_model = tf.stop_gradient(vgg_trained_model) #Just use it in case of transfer learning without fine tunning\n",
        "  \n",
        "  \n",
        "#   print(graph.get_operations())\n",
        "  return vgg_trained_model, graph\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RpqQZnC8MVlL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Creating the model\n",
        "The function  \"transfer_learning_model\" is responsible for creating the model that will be used for recognizing the CIFAR10 images.\n",
        "\n",
        "The first scope (\"placeholders_variables\") defines:\n",
        "* ** input images** -  the images that will feed the model\n",
        "* **labels** - each image that feeds the input placeholder,  need to have a correspondent label, wich will feed this  placeholder when the loss was calculated.\n",
        "* **dropout_keep** - it defines a percent of neurons that will not be activated in each fully connected layer. The number to be fed is between 0 and 1.\n",
        "* **global_step** - As the train process is running, this variable stores the value of the current step. This value can be used for saving a checkpoint in an specific step, and, when restored, all the model continues the training process from this point/step.\n",
        "* **learning rate** - it defines the learning rate to be used by the optimizer. In this case, the global step is used in order to provide a decay point even whether the training is restarted or not. It starts from an initial learning rate and decays according to an specific rate, with each number of steps.\n",
        "\n",
        "These parameters are able to  influence directly the success of the training, so they are defined as hyperparameters of the model (class \"Hyperparameters\"), and must be treated and chosen carefully.\n"
      ]
    },
    {
      "metadata": {
        "id": "EjFZLWmlMWgr",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def transfer_learning_model(params = None, fine_tunning = False, bottleneck = False):\n",
        "   \n",
        "    if params is None:\n",
        "      params = Hyperparameters()\n",
        "      \n",
        "    with tf.name_scope('placeholders_variables'):\n",
        "        input_images = tf.placeholder(tf.float32, shape=[None,params.image_size, params.image_size, params.image_channels], name='input')\n",
        "        labels = tf.placeholder(tf.float32, shape=[None, params.num_classes], name='labels')\n",
        "#         reshaped_images = tf.reshape(input_images, [-1, params.image_size, params.image_size, params.image_channels], name='images')\n",
        "        dropout_keep  =  tf.placeholder(tf.float32, name='dropout_keep')\n",
        "        global_step = tf.train.get_or_create_global_step()\n",
        "        learning_rate = tf.train.exponential_decay(params.initial_learning_rate, global_step, \n",
        "                                               params.decay_steps,params.decay_rate, staircase=True)       \n",
        "    \n",
        "\n",
        "    with tf.name_scope('vgg16'):\n",
        "     # Create a VGG16 model and reuse its weights.\n",
        "      vgg16_out,_ = get_vgg16(input_images=input_images,cut_layer = params.cut_layer, fine_tunning = fine_tunning)\n",
        "      \n",
        "    with tf.name_scope(\"flatten\"):\n",
        "      flatten = tf.layers.flatten(vgg16_out, name=\"flatten\")\n",
        "    \n",
        "    if (not fine_tunning) and bottleneck:\n",
        "        out_list = flatten.shape.as_list()\n",
        "        BOTTLENECK_TENSOR_SIZE = np.prod(out_list[1:]) # All input layer size, less the batch size\n",
        "        with tf.name_scope('bottleneck'):\n",
        "            bottleneck_tensor = flatten\n",
        "            bottleneck_input = tf.placeholder(tf.float32,\n",
        "            shape=[None, BOTTLENECK_TENSOR_SIZE],\n",
        "            name='InputPlaceholder')\n",
        "\n",
        "        with tf.name_scope('fully_conn'):\n",
        "          logits = fc_model(bottleneck_input, params.hidden_layers) #Create a fully connect model which will be feed by the vgg16\n",
        "    \n",
        "    else:\n",
        "      with tf.name_scope('fully_conn'):\n",
        "          logits = fc_model(flatten, params.hidden_layers) #Create a fully connect model which will be feed by the vgg16\n",
        "\n",
        "        \n",
        "\n",
        "    with tf.name_scope('loss'):\n",
        "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=labels))\n",
        "#         loss = regularize(loss)\n",
        "        tf.summary.scalar(\"loss\", loss)\n",
        "\n",
        "\n",
        "    with tf.name_scope('sgd'):\n",
        "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "        with tf.control_dependencies(update_ops):\n",
        "            optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
        "\n",
        "    with tf.name_scope('train_accuracy'):\n",
        "        acc = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
        "        acc = tf.reduce_mean(tf.cast(acc, tf.float32))\n",
        "        tf.summary.scalar(\"accuracy\", acc)\n",
        "   \n",
        "    \n",
        "    predictions = {\n",
        "                   \"classes\": tf.argmax(logits, 1),\n",
        "                   \"probs\" :  tf.nn.softmax(logits), \n",
        "                   \"labels\": tf.argmax(labels, 1)\n",
        "                   }\n",
        "    model = {\n",
        "              \"global_step\": global_step,\n",
        "              \"images\": input_images,\n",
        "              \"labels\": labels,    \n",
        "              \"loss\" : loss,\n",
        "              \"optimizer\": optimizer,\n",
        "              \"accuracy\": acc,\n",
        "              \"predictions\":predictions,\n",
        "              \"keep\": dropout_keep\n",
        "          }\n",
        "\n",
        "    \n",
        "    if (not fine_tunning) and bottleneck:\n",
        "        model.update({\"bottleneck_tensor\":bottleneck_tensor})\n",
        "        model.update({\"bottleneck_input\":bottleneck_input})\n",
        "     \n",
        " \n",
        "        \n",
        "    return model\n",
        "        \n",
        "def get_fc_weights(w_inputs, w_output, id=0):\n",
        "        weight= tf.Variable(tf.truncated_normal([w_inputs, w_output]), name=\"{}/weight\".format(id))\n",
        "        bias =  tf.Variable(tf.truncated_normal([w_output]), name=\"{}/bias\".format(id))\n",
        "        return weight, bias  \n",
        "\n",
        "def logits_layer(fc_layer, n_classes):\n",
        "        out_shape = fc_layer.shape.as_list()\n",
        "        w, b = get_fc_weights(np.prod(out_shape[1:]), n_classes, \"logits/weight\")\n",
        "        logits = tf.add(tf.matmul(fc_layer, w), b, name=\"logits\")\n",
        "        return logits\n",
        "      \n",
        "def fc_layer(input_layer, number_of_units, keep = None, layer_id = \"fc\"):\n",
        "        pl_list = input_layer.shape.as_list()\n",
        "        input_size = np.prod(pl_list[1:])\n",
        "        \n",
        "        w, b = get_fc_weights(input_size, number_of_units, layer_id)  \n",
        "        fc_layer = tf.matmul(input_layer, w, name=\"{}/matmul\".format(layer_id))\n",
        "        fc_layer = tf.nn.bias_add(fc_layer, b, name=\"{}/bias-add\".format(layer_id))\n",
        "       \n",
        "        if keep is not None:\n",
        "          fc_layer = tf.nn.dropout(fc_layer, keep, name=\"{}/dropout\".format(layer_id))\n",
        "        else:\n",
        "          print(\"Dropout was disabled.\")\n",
        "        \n",
        "        fc_layer = tf.nn.relu(fc_layer, name=\"{}/relu\".format(layer_id))\n",
        "        return fc_layer\n",
        "      \n",
        "def regularize(loss, type = 1, scale = 0.005, scope = None):\n",
        "        if type == 1:\n",
        "            regularizer = tf.contrib.layers.l1_regularizer( scale=scale, scope=scope)\n",
        "        else:\n",
        "            regularizer = tf.contrib.layers.l2_regularizer( scale=scale, scope=scope)\n",
        "                \n",
        "        weights = tf.trainable_variables() # all vars of your graph\n",
        "        regularization_penalty = tf.contrib.layers.apply_regularization(regularizer, weights)\n",
        "        regularized_loss = loss + regularization_penalty\n",
        "        return regularized_loss\n",
        "\n",
        "def fc_model(flatten, hidden_layers = [512], keep = None):\n",
        "        fc = flatten\n",
        "        id = 1\n",
        "        for num_neurons in hidden_layers:\n",
        "          fc = fc_layer(fc, num_neurons, keep,  \"fc{}\".format(id) )\n",
        "          id = id+1\n",
        "          \n",
        "        logits = logits_layer(fc, params.num_classes)\n",
        "        return logits\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d8MnlJ4QNrFf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Creating a session\n",
        "The function \"create_monitored_session\" creates a tensorflow session able to restore weights and/or save them. The parameter \"checkpoint_dir\" represents where the weights were saved or where one wants they be saved. All the save/restore process is made automatically by tensorflow.\n",
        "\n",
        "As default, tensorflow allocate all GPU memory in the first called to the session run, thus the \"tf.ConfigProto()\", by setting the \"True\" to the \"gpu_options.allow_growth\", allows the gradual increase of memory. In other words, it allows to allocate the GPU memory by demanding. This is  important mainly when more than one training or prediction process is running on the same GPU.\n"
      ]
    },
    {
      "metadata": {
        "id": "ldO41ReMNrXJ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def create_monitored_session(model,iter_per_epoch, checkpoint_dir):\n",
        "  config = tf.ConfigProto()\n",
        "  config.gpu_options.allow_growth = True\n",
        "\n",
        "\n",
        "  sess = tf.train.MonitoredTrainingSession(checkpoint_dir=checkpoint_dir,\n",
        "                                      save_checkpoint_secs=120,\n",
        "                                      log_step_count_steps=iter_per_epoch,\n",
        "                                      save_summaries_steps=iter_per_epoch,\n",
        "                                      config=config) \n",
        "  return sess"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EaAGt5L8O--z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Testing the model\n",
        "The function \"test\" is responsible for applying the test dataset through the trained model. Thus, it is possible to monitor the model progress. This function could be change in order to do a validation test, which uses the validation dataset,  rather than just a test. It would be helpful for problems that do not release a labeled test dataset."
      ]
    },
    {
      "metadata": {
        "id": "hBjCG-7ALuas",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def test(sess, model,input_data_placeholder, data, labels, batch_size = 128):\n",
        "            global_accuracy = 0\n",
        "            predictions = {\n",
        "                           \"classes\":[],\n",
        "                           \"probs\":[],\n",
        "                           \"labels\":[]\n",
        "                          }\n",
        "           \n",
        "            size = len(data)//batch_size\n",
        "            indices = list(range(len(data)))\n",
        "            \n",
        "            for i in range(size+1):\n",
        "               \n",
        "                begin = i*batch_size\n",
        "                end = (i+1)*batch_size\n",
        "                end = len(data) if end >= len(data) else end\n",
        "                \n",
        "                next_bach_indices = indices[begin:end]\n",
        "                batch_xs = data[next_bach_indices]\n",
        "                batch_ys = labels[next_bach_indices]\n",
        "                \n",
        "                pred = sess.run(model[\"predictions\"],\n",
        "                    feed_dict={input_data_placeholder: batch_xs, model[\"labels\"]: batch_ys, model[\"keep\"]:1.0})\n",
        "                \n",
        "                predictions[\"classes\"].extend(pred[\"classes\"])\n",
        "                predictions[\"probs\"].extend(pred[\"probs\"])\n",
        "                predictions[\"labels\"].extend(pred[\"labels\"])\n",
        "            \n",
        "            \n",
        "            correct = list (map(lambda x,y: 1 if x==y else 0, predictions[\"labels\"] , predictions[\"classes\"]))\n",
        "            acc = np.mean(correct ) *100\n",
        "            \n",
        "            mes = \"--> Test accuracy: {:.2f}% ({}/{})\"\n",
        "            print(mes.format( acc, sum(correct), len(data)))\n",
        "            \n",
        "            return predictions\n",
        "                      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CXZQdFvFOEwx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Training the model: the mainly function\n",
        "The \"train\" function is responsible for training the model. It starts checking the hyperparameters and resetting the default graph.  After, the dataset is loaded by using the class \"util\". The next step consists of  creating the model, where the tensorflow graph is created. Now, a monitored season is created too. This kind of session will save and restore the model automatically, which will be very important when an unexpected event occurs and the model stop the training (such as a power outage or when the Google Colab finishes the session during the training).\n",
        "\n",
        "With the model and the session created, you are able, if you want, to generate or load the bottlenecks files. This is what the next lines are doing. One of the most important results of theses lines is  obtaining the tensor \"input_data_placeholder\". It is important because when the bottleneck option is chosen, the \"feed_dict\" must feed the placeholder of the \"bottleneck\" rather than the one that feeds input to the VVG16. Thus, if the bottleneck is chosen, the input placeholder will be the \"model[bottleneack_input]\", else, it will be the input tensor of the vgg16, \"model[images]\".\n",
        "\n",
        "In the  the beginning of each epoch, in order to ensure the randomness of the baths,  a list containing the dataset indices is shuffled. So, at every batch, a new range of indices is taken and the batch may feed the placeholder.\n",
        "\n",
        "Therefore, the session can call the optimizer and training the model. Finally, the last step is to call the test for checking the result of the training for the present epoch.\n"
      ]
    },
    {
      "metadata": {
        "id": "SoXSuCQ3OFWN",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def train(params = None):\n",
        "    if params is None:\n",
        "      params = Hyperparameters()\n",
        "      \n",
        "    tf.reset_default_graph()\n",
        "\n",
        "     \n",
        "    train_data, train_labels = utils.get_data_set(\"train\")\n",
        "    train_data, train_labels = utils.data_augmentation(train_data, train_labels)\n",
        "    \n",
        "    test_data, test_labels = utils.get_data_set(\"test\")  \n",
        "    \n",
        "    \n",
        "    model = transfer_learning_model(params, params.fine_tunning, params.bottleneck)\n",
        "   \n",
        "    steps_per_epoch = int(math.ceil(len(train_data) /  params.batch_size))\n",
        "    sess = create_monitored_session(model,steps_per_epoch, params.check_points_path)\n",
        "    \n",
        "    \n",
        "    if (not params.fine_tunning) and params.bottleneck:\n",
        "        indices = list( range(len(train_data)) )\n",
        "        shuffle(indices)\n",
        "        \n",
        "        shuffled_data = train_data[indices]\n",
        "        shuffled_labels = train_labels[indices]\n",
        "        \n",
        "        bottleneck_train = utils.get_or_generate_bottleneck(sess, model, \"bottleneck_vgg16_{}_train\".format(params.cut_layer), shuffled_data, shuffled_labels)\n",
        "        bottleneck_test = utils.get_or_generate_bottleneck(sess, model, \"bottleneck_vgg16_{}_test\".format(params.cut_layer), test_data, test_labels)\n",
        "        \n",
        "        \n",
        "        train_data, train_labels  = bottleneck_train[\"data\"], bottleneck_train[\"labels\"]\n",
        "        test_data, test_labels = bottleneck_test[\"data\"], bottleneck_test[\"labels\"]\n",
        "        del bottleneck_train, bottleneck_test\n",
        "        \n",
        "        \n",
        "        input_data_placeholder = model[\"bottleneck_input\"]\n",
        "        \n",
        "        \n",
        "    else:\n",
        "        input_data_placeholder = model[\"images\"]\n",
        "        \n",
        "        \n",
        "    \n",
        "    indices = list( range(len(train_data)) )\n",
        "    msg = \"--> Global step: {:>5} - Last batch acc: {:.2f}% - Batch_loss: {:.4f} - ({:.2f}, {:.2f}) (steps,images)/sec\"\n",
        "    \n",
        "    for epoch in range(params.num_epochs):\n",
        "        start_time = time()\n",
        "        \n",
        "        print(\"\\n*************************************************************\")\n",
        "        print(\"Epoch {}/{}\".format(epoch+1,params.num_epochs))\n",
        "        \n",
        "        shuffle(indices)  \n",
        "        for s in range(steps_per_epoch):\n",
        "          \n",
        "            indices_next_batch = indices[s *  params.batch_size : (s+1) * params.batch_size]\n",
        "            batch_data = train_data[indices_next_batch]\n",
        "            batch_labels = train_labels[indices_next_batch]\n",
        "            \n",
        "            _, batch_loss, batch_acc,step = sess.run(\n",
        "                [model[\"optimizer\"], model[\"loss\"], model[\"accuracy\"], model[\"global_step\"],],\n",
        "                feed_dict={input_data_placeholder: batch_data, model[\"labels\"]: batch_labels, model[\"keep\"]:params.keep})\n",
        "        \n",
        "        duration = time() - start_time\n",
        "\n",
        "        print(msg.format(step,  batch_acc*100, batch_loss, (steps_per_epoch / duration), (steps_per_epoch*params.batch_size / duration) ))\n",
        "\n",
        "        \n",
        "        _ = test(sess, model, input_data_placeholder, test_data, test_labels )\n",
        "    \n",
        "    predictions = test(sess, model, input_data_placeholder, test_data, test_labels )\n",
        "\n",
        "    sess.close()\n",
        "    \n",
        "    class_names = [\"airplane\",\"automobile\",\"bird\",\"cat\",\"deer\",\"dog\",\"frog\",\"horse\",\"ship\",\"truck\"] \n",
        "    utils.generate_confusion_matrix(predictions, class_names)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YN2o7cb8OSgp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This part of code instantiates a \"Hyperparameters\" class, changes it and passes it as parameter to the train function. Thus, the training can be started."
      ]
    },
    {
      "metadata": {
        "id": "7_0NbT5ROSzE",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  params = Hyperparameters()\n",
        "  params.num_epochs = 200\n",
        "  params.hidden_layers = [512]\n",
        "  params.initial_learning_rate = 1e-4\n",
        "  params.cut_layer = \"pool3\"\n",
        "\n",
        "  train(params)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Iel8lO7CGtzy",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
